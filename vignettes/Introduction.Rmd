---
title: "Introduction to StatComp18055"
author: "18055"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp18055}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp18055__ is a simple R package which includes four R functions for the 'Statistical Computing' course: _CVM.test_ (test for equal distributions in the univariate case), _my.pcauchy_ (compute the cdf of the Cauchy distribution), _my.chisq.test_(a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values) and _my.table_(a faster version of table() for the case of an input of two integer vectors with no missing values).

The R package 'microbenchmark' can be used to benchmark the functions _my.chisq.test_ and _my.table_.


## _CVM.test_

The function CVM.test() chooses Cramer-von Mises statistic to measure the difference between two distributions and tests for equal distributions in the univariate case. The source R code for _CVM.test_ is as follows:
```{r,eval=TRUE}
CVM.test <- function(x,y,R) {
  CVM.stat <- function(x,y) {
    n <- length(x)
    m <- length(y)
    F_n <- ecdf(x)
    G_m <- ecdf(y)
    T <- ((m*n)/(m+n)^2)*(sum((F_n(x)-G_m(x))^2) + sum((F_n(y)-G_m(y))^2)) # the Cramer-Von Mises (CVM) statistic
    return(T)
  } # use Cramer-von Mises statistic to measure the difference between two distributions
  n1 <- length(x)
  n2 <- length(y)
  n <- n1 + n2
  z <- c(x, y) # pooled sample
  K <- 1:n
  reps <- numeric(R) # storage for replicates
  t0 <- CVM.stat(x,y) # the observed statistic t0
  for (i in 1:R) { # permutation samples
    k <- sample(K, size = n1, replace = FALSE)
    x1 <- z[k]
    y1 <- z[-k] # complement of x1
    reps[i] <- CVM.stat(x1, y1) # the Cramer-Von Mises (CVM) statistic T
  }
  p <- mean(c(t0, reps) >= t0)
  print(p)
}
```

We can apply the test to the data in Examples 8.1 and 8.2(Statistical Computing with R):

```{r,eval=TRUE}
# obtain data
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)
set.seed(1)
CVM.test(x,y,R=999)
```

The value of p we obtained is 0.421, so the null hypothesis that distributions are equal is not rejected.


## _my.pcauchy_

The function my.pcauchy() computes the cdf of the Cauchy distribution. The source R code for _my.pcauchy_ is as follows:
```{r,eval=TRUE}
my.pcauchy <- function (x, eta, theta) {
  stopifnot(theta > 0)
  my.dcauchy <- function (x, eta, theta) {
    return(1/(theta*pi*(1 + ((x - eta)/theta)^2)))
  } # the density function of the Cauchy distribution
  integral <- function (x) {
    my.dcauchy(x, eta, theta)
  }
  return(integrate(integral, lower = -Inf, upper = x, rel.tol=.Machine$double.eps^0.25)$value)
} # the value of the integrand is the cdf of the Cauchy distribution
```

We can compare our results obtained by my.pcauchy() to the results obtained by the R function pcauchy():

```{r,eval=TRUE}
# Cauchy(0,1)
xs <- seq(-4, 5)
names(xs) <-c(seq(-4, 5))
my.pcauchy.val <- sapply(xs, function(x) my.pcauchy(x, 0, 1))
pcauchy.val <- sapply(xs, function(x) pcauchy(x, 0, 1))
round(rbind(my.pcauchy=my.pcauchy.val, pcauchy=pcauchy.val), 4)
# Cauchy(1,2)
my.pcauchy.val <- sapply(xs, function(x) my.pcauchy(x, 1, 2))
pcauchy.val <- sapply(xs, function(x) pcauchy(x, 1, 2))
round(rbind(my.pcauchy=my.pcauchy.val, pcauchy=pcauchy.val), 4)
```

Setting different parameters of Cauchy distribution and comparing the results obtained by my.pcauchy() to the results obtained by pcauchy(), we can clearly find that the two results are identical. So the function we write is reasonable.


## _my.chisq.test_

The function my.chisq.test() is a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. The source R code for _my.chisq.test_ is as follows:
```{r,eval=TRUE}
my.chisq.test <- function(x, y) {
  total <- sum(x) + sum(y)
  rowsum_x <- sum(x)
  rowsum_y <- sum(y)
  chistat <- 0
  expected <- function(colsum, rowsum, total) {
    (colsum / total) * (rowsum / total) * total
  } # the expected (theoretical) count of type i
  chi_stat <- function(observed, expected) {
    ((observed - expected) ^ 2) / expected
  } # a measure of the approximation between observed value and expected value
  for (i in seq_along(x)) {
    colsum <- x[i] + y[i]
    expected_x <- expected(colsum, rowsum_x, total)
    expected_y <- expected(colsum, rowsum_y, total)
    chistat <- chistat + chi_stat(x[i], expected_x)
    chistat <- chistat + chi_stat(y[i], expected_y)
  }
  chistat
} # code from the mathematical definition
```

We can apply the test to the data in Examples7.4.5(Probability Theory and Mathematical Statistics):

```{r,eval=TRUE}
my.chisq.test(c(367,342,266,329),c(56,40,20,16))
```

We can compare our results obtained by my.chisq.test() to the results obtained by the R function chisq.test():

```{r,eval=TRUE}
# validate the function my.chisq.test() is reasonable
chisq.test(as.table(rbind(c(367,342,266,329),c(56,40,20,16))))
# Validate the function my.chisq.test() is faster than chisq.test()
print(microbenchmark::microbenchmark(
  my.chisq.test = my.chisq.test(c(367,342,266,329),c(56,40,20,16)),
  chisq.test = chisq.test(as.table(rbind(c(367,342,266,329),c(56,40,20,16))))
))
```

From the results above, we can find that: when the input is two numeric vectors with no missing values, the function my.chisq.test() can get the same chi-squared test statistic value as chisq.test(), and my.chisq.test() only computes the chi-square test statistic. Using the microbenchmark package to compare how long each function takes to run, we can find that my.chisq.test() is obviously faster than chisq.test().


## _my.table_

The function my.table() is a faster version of table() for the case of an input of two integer vectors with no missing values. The source R code for _my.table_ is as follows:
```{r,eval=TRUE}
my.table <- function(x, y) {
  x_unique <- unique(x) # extract unique elements of x
  y_unique <- unique(y) # extract unique elements of y
  mat <- matrix(0L, length(x_unique), length(y_unique)) # storage for replicates
  for (i in seq_along(x)) {
    mat[which(x_unique == x[[i]]), which(y_unique == y[[i]])] <-
      mat[which(x_unique == x[[i]]),  which(y_unique == y[[i]])] + 1L
  } # build a contingency table of the counts at each combination of factor levels
  # optimize the form of output
  dimnames <- list(x_unique, y_unique)
  names(dimnames) <- as.character(as.list(match.call())[-1])
  tab <- array(mat, dim = dim(mat), dimnames = dimnames)
  class(tab) <- "table"
  tab
}
```

We can use an example to compare our results obtained by my.table() to the results obtained by the R function table():

```{r,eval=TRUE}
x <- c(1, 2, 2, 3, 1, 3, 3, 2)
y <- c(1, 1, 2, 1, 1, 1, 1, 2)
# validate the function my.table() is reasonable
my.table(x, y)
table(x, y)
# validate the function my.table() is faster than table()
print(microbenchmark::microbenchmark(
  my.table = my.table(x, y),
  table = table(x, y)
))
```

From the results above, we can find that: when the input is two integer vectors with no missing values, the function my.table() can get the same contingency table as table(). Using the microbenchmark package to compare how long each function takes to run, we can find that my.table() is obviously faster than table().
